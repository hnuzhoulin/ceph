#Ceph 介绍
# 硬件推荐 #
ceph被设计运行于商业硬件上,让经济地来管理维护PB级别的数据集群成为可行.

    注意:


## CPU ##

使用性能强悍CPU的ceph元数据服务器能够动态分配负载.所以你的元数据服务器必须注意(CPU的)处理能力(比如:四核或者更好的CPU).ceph的OSD运行着RADOS服务,通过CRUSH算法计算数据分布,复制,和维护他们

    #!/bin/bash



## 内存 ##



## 数据存储 ##
认真规划你的数据存储配置.当规划数据存储时,考虑成本和性能具有重大的意义.从多实例中,通过对单一驱动进行并发系统操作和并发请求读写操作能够不错的表现出性能.文件系统的限制也要考虑进来.btrfs能够同时记录日志和写数据,xfs和ext4文件系统却不能做到,但在生产中btrfs还不足够稳定.



>重要提醒:既然ceph必须写完所有数据到日志后才回复确认(至少对于xfs和ext4文件系统是这样的),所以权衡日志和OSD的性能真的是很重要!


**机械硬盘**

OSD服务器必须有足够的硬盘空间来存储对象数据.考虑到大硬盘的每GB的价格优势,我们建议每个硬盘最小1TB.我们建议通过GB的数量来分开硬盘机械硬盘以达到每GB的成本,因为大的机械硬盘对每GB的成本有重大影响.比如1TB的硬盘的价格在75美元,每GB的成本在0.0732美元(75美元/1024=0.0732美元),对比于一个3TB价格在150美元的硬盘,每GB的价格在0.05美元(150美元/3073=0.0488美元).通过前面的例子,使用1TB的硬盘使得成本增加了40%,实质上大大减少了集群的成本效益(就是增加了成本).同样,越大的存储驱动能力,每个OSD进程需要更多的内存,尤其在重新计算负载,回填,回复的时候.通常的规则就是1TB的存储空间需要1GB内存.
>注意:运行多个OSD在同一个硬盘不是一个好主意,且这跟分区没关系.

>注意:运行一个OSD和一个mon或者元数据服务器在同一个硬盘上不是一个好主意,且这跟分区没关系.

存储驱动主要受限制于seek时间,访问时间,读和写时间,以其总的吞吐量.这些物理限制都将影响系统性能,尤其在恢复的时候.我们建议为操作系统和软件使用专门的机械硬盘,在主机中一个OSD使用一个机械硬盘.
大多数"查看 OSDs"问题都是由于在一个系统里面,一个机械硬盘上运行多OSDs,和/或者是运行很多日志服务器导致的.在一个小的集群中寻找性能故障的成本极可能超过单独买额外硬盘驱动的成本,通过避免过度负载OSD存储的机械硬盘的诱惑,你可以加速你的集群设计规划.

你也许会在一个硬盘机械硬盘上运行多个OSD进程,但是这个可能导致资源争用和减少总的吞吐量.你可以存一个日志和对象数据再用一个机械硬盘上,但是这个可能增加一个日志写和回复给客户端的时间.在回复写之前,ceph必须先写到日志中.btrfs文件系统可以同时写日志数据和对象数据,然后xfs和ext4不能.


ceph最佳实践指出你应该将操作系统,OSD数据和OSD日志运行在单独的机械硬盘上.


**固态硬盘**

当加速吞吐量时,通过固态硬盘来减少随机访问时间和读延时是一个改进性能的机会.
